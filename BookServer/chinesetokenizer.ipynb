{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a08c364-e059-40ba-8924-fa186cef08af",
   "metadata": {},
   "source": [
    "# 中文分词测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b65af23-a176-44aa-a8fe-dc879660f4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"九二：眇能视，利幽人之贞。九二：履道坦坦，幽人贞吉。六五：密云不雨，自我西郊；公弋取彼在穴。九五：东邻杀牛，不如西邻之禴祭，实受其福，（吉）。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891df432-7d3c-42be-b751-5b0aa5351e4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Jieba分词\n",
    "速度比较快，对whoosh的支持也比较好，有专门的ChineseAnalyzer，但分词测试有些不太准确，导致建立索引后搜索不到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428d3484-fb65-4649-9d58-8e04f185ed8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 精准分词模式: 九二/ ：/ 眇/ 能视/ ，/ 利幽/ 人之贞/ 。/ 九二/ ：/ 履道/ 坦坦/ ，/ 幽人贞吉/ 。/ 六五/ ：/ 密云不雨/ ，/ 自我/ 西郊/ ；/ 公弋取/ 彼/ 在/ 穴/ 。/ 九五/ ：/ 东邻/ 杀/ 牛/ ，/ 不如/ 西邻/ 之/ 禴/ 祭/ ，/ 实受/ 其福/ ，/ （/ 吉/ ）/ 。\n",
      "\n",
      "2. 搜索引擎模式: 九二/ ：/ 眇/ 能视/ ，/ 利幽/ 人之贞/ 。/ 九二/ ：/ 履道/ 坦坦/ ，/ 幽人/ 幽人贞吉/ 。/ 六五/ ：/ 密云/ 密云不雨/ ，/ 自我/ 西郊/ ；/ 公弋取/ 彼/ 在/ 穴/ 。/ 九五/ ：/ 东邻/ 杀/ 牛/ ，/ 不如/ 西邻/ 之/ 禴/ 祭/ ，/ 实受/ 其福/ ，/ （/ 吉/ ）/ 。\n",
      "\n",
      "3. jiebaChineseAnalyzer: 九二/ 眇/ 能视/ 利幽/ 人之贞/ 九二/ 履道/ 坦坦/ 幽人/ 幽人贞吉/ 六五/ 密云/ 密云不雨/ 自我/ 西郊/ 公弋取/ 彼/ 在/ 穴/ 九五/ 东邻/ 杀/ 牛/ 不如/ 西邻/ 之/ 禴/ 祭/ 实受/ 其福/ 吉\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from jieba.analyse import ChineseAnalyzer\n",
    "\n",
    "analyzer = ChineseAnalyzer()\n",
    "\n",
    "# 幽人的分词有问题\n",
    "seg_list = jieba.cut(text)\n",
    "print(\"1. 精准分词模式:\", \"/ \".join(seg_list)) \n",
    "\n",
    "seg_list = jieba.lcut_for_search(text)\n",
    "print(\"\\n2. 搜索引擎模式:\", \"/ \".join(seg_list)) \n",
    "\n",
    "# ChineseAnalyzer内部使用的是搜索引擎模式\n",
    "tokens = [t.text for t in analyzer(text)]\n",
    "print(\"\\n3. jiebaChineseAnalyzer:\", \"/ \".join(tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a1039-99bb-421b-a250-f4b566be4d83",
   "metadata": {},
   "source": [
    "## stanza分词\n",
    "经过测试，对文言文的分词还不错，就是速度太慢，用它来生成一个适配whoosh的中文分词工具，好像还有死锁的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e501af-5623-470f-8103-6775764cd3ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410f7423-02bf-4a1d-b6de-365a39c2afe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 09:23:48 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2023-08-25 09:23:50 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | gsdsimp   |\n",
      "| pos          | gsdsimp   |\n",
      "| lemma        | gsdsimp   |\n",
      "| constituency | ctb       |\n",
      "| depparse     | gsdsimp   |\n",
      "| sentiment    | ren       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-08-25 09:23:50 INFO: Using device: cpu\n",
      "2023-08-25 09:23:50 INFO: Loading: tokenize\n",
      "2023-08-25 09:23:50 INFO: Loading: pos\n",
      "2023-08-25 09:23:50 INFO: Loading: lemma\n",
      "2023-08-25 09:23:50 INFO: Loading: constituency\n",
      "2023-08-25 09:23:51 INFO: Loading: depparse\n",
      "2023-08-25 09:23:51 INFO: Loading: sentiment\n",
      "2023-08-25 09:23:51 INFO: Loading: ner\n",
      "2023-08-25 09:23:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('九', 2, 'nummod')\n",
      "('二', 7, 'nmod:tmod')\n",
      "('：', 7, 'punct')\n",
      "('眇', 7, 'nsubj')\n",
      "('能视', 7, 'advcl')\n",
      "('，', 7, 'punct')\n",
      "('利', 0, 'root')\n",
      "('幽人', 10, 'amod')\n",
      "('之', 8, 'case')\n",
      "('贞', 7, 'obj')\n",
      "('。', 7, 'punct')\n",
      "('九', 2, 'nummod')\n",
      "('二', 0, 'root')\n",
      "('：', 2, 'punct')\n",
      "('履道', 2, 'appos')\n",
      "('坦坦', 4, 'flat:name')\n",
      "('，', 7, 'punct')\n",
      "('幽人', 4, 'appos')\n",
      "('贞吉', 7, 'flat:name')\n",
      "('。', 2, 'punct')\n",
      "('六', 2, 'nummod')\n",
      "('五', 7, 'nmod:tmod')\n",
      "('：', 7, 'punct')\n",
      "('密云', 7, 'nsubj')\n",
      "('不雨', 7, 'advcl')\n",
      "('，', 7, 'punct')\n",
      "('自我', 0, 'root')\n",
      "('西郊', 7, 'obj')\n",
      "('；', 7, 'punct')\n",
      "('公弋', 11, 'nsubj')\n",
      "('取彼', 7, 'parataxis')\n",
      "('在', 11, 'mark')\n",
      "('穴', 11, 'obj')\n",
      "('。', 7, 'punct')\n",
      "('九', 2, 'nummod')\n",
      "('五', 13, 'nmod:tmod')\n",
      "('：', 13, 'punct')\n",
      "('东邻', 5, 'nmod')\n",
      "('杀牛', 13, 'nsubj')\n",
      "('，', 13, 'punct')\n",
      "('不如', 13, 'advcl')\n",
      "('西邻', 11, 'nmod')\n",
      "('之', 8, 'case')\n",
      "('禴', 11, 'compound')\n",
      "('祭', 7, 'obj')\n",
      "('，', 13, 'punct')\n",
      "('实受', 0, 'root')\n",
      "('其', 15, 'compound')\n",
      "('福', 13, 'obj')\n",
      "('，', 13, 'punct')\n",
      "('（', 18, 'punct')\n",
      "('吉', 15, 'appos')\n",
      "('）', 18, 'punct')\n",
      "('。', 13, 'punct')\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "#stanza.download('zh')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('zh', download_method=stanza.DownloadMethod.REUSE_RESOURCES) # This sets up a default neural pipeline in English\n",
    "doc = nlp(text)\n",
    "for sentence in doc.sentences:\n",
    "    sentence.print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b8195d0-28a7-41e3-80be-3388fccf5942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from whoosh.analysis import RegexAnalyzer, LowercaseFilter, StopFilter, StemFilter\n",
    "from whoosh.analysis import Tokenizer, Token\n",
    "from whoosh.lang.porter import stem\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "STOP_WORDS = frozenset(('a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'can',\n",
    "                        'for', 'from', 'have', 'if', 'in', 'is', 'it', 'may',\n",
    "                        'not', 'of', 'on', 'or', 'tbd', 'that', 'the', 'this',\n",
    "                        'to', 'us', 'we', 'when', 'will', 'with', 'yet',\n",
    "                        'you', 'your', '的', '了', '和'))\n",
    "\n",
    "accepted_chars = re.compile(r\"[\\u4E00-\\u9FD5]+\")\n",
    "\n",
    "class StanzaChineseTokenizer(Tokenizer):\n",
    "    nlp = None\n",
    "\n",
    "    def __call__(self, text, **kargs):\n",
    "        #stanza.download('zh')       # This downloads the English models for the neural pipeline\n",
    "        if self.nlp is None:\n",
    "            self.nlp = stanza.Pipeline('zh', download_method=stanza.DownloadMethod.REUSE_RESOURCES) # This sets up a default neural pipeline in English\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        token = Token()\n",
    "        for sentence in doc.sentences:\n",
    "            #print(sentence)\n",
    "            for tok in sentence.tokens:\n",
    "                text = ''\n",
    "                start_char = tok._start_char\n",
    "                end_char = tok._end_char\n",
    "                if tok.words:  # not-yet-processed MWT can leave empty tokens\n",
    "                    for word in tok.words:\n",
    "                        start_char = word._start_char\n",
    "                        end_char = word._end_char\n",
    "                        text = word._text\n",
    "\n",
    "                if not accepted_chars.match(text) and len(text) <= 1:\n",
    "                    continue\n",
    "                token.original = token.text = text\n",
    "                token.pos = start_char\n",
    "                token.startchar = start_char\n",
    "                token.endchar = end_char\n",
    "                yield token\n",
    "\n",
    "\n",
    "def StanzaChineseAnalyzer(stoplist=STOP_WORDS, minsize=1, stemfn=stem, cachesize=50000):\n",
    "    return (StanzaChineseTokenizer() | LowercaseFilter() |\n",
    "            StopFilter(stoplist=stoplist, minsize=minsize) |\n",
    "            StemFilter(stemfn=stemfn, ignore=None, cachesize=cachesize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecf1e1ce-fc28-447a-91fb-1307ca1847f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 10:32:23 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2023-08-25 10:32:25 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | gsdsimp   |\n",
      "| pos          | gsdsimp   |\n",
      "| lemma        | gsdsimp   |\n",
      "| constituency | ctb       |\n",
      "| depparse     | gsdsimp   |\n",
      "| sentiment    | ren       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-08-25 10:32:25 INFO: Using device: cpu\n",
      "2023-08-25 10:32:25 INFO: Loading: tokenize\n",
      "2023-08-25 10:32:25 INFO: Loading: pos\n",
      "2023-08-25 10:32:25 INFO: Loading: lemma\n",
      "2023-08-25 10:32:25 INFO: Loading: constituency\n",
      "2023-08-25 10:32:25 INFO: Loading: depparse\n",
      "2023-08-25 10:32:26 INFO: Loading: sentiment\n",
      "2023-08-25 10:32:26 INFO: Loading: ner\n",
      "2023-08-25 10:32:26 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StanzaChineseAnalyzer: 九/ 二/ 眇/ 能视/ 利/ 幽人/ 之/ 贞/ 九/ 二/ 履道/ 坦坦/ 幽人/ 贞吉/ 六/ 五/ 密云/ 不雨/ 自我/ 西郊/ 公弋/ 取彼/ 在/ 穴/ 九/ 五/ 东邻/ 杀牛/ 不如/ 西邻/ 之/ 禴/ 祭/ 实受/ 其/ 福/ 吉\n",
      "九, start_char: 0, end_char: 1.\n",
      "二, start_char: 1, end_char: 2.\n",
      "眇, start_char: 3, end_char: 4.\n",
      "能视, start_char: 4, end_char: 6.\n",
      "利, start_char: 7, end_char: 8.\n",
      "幽人, start_char: 8, end_char: 10.\n",
      "之, start_char: 10, end_char: 11.\n",
      "贞, start_char: 11, end_char: 12.\n",
      "九, start_char: 13, end_char: 14.\n",
      "二, start_char: 14, end_char: 15.\n",
      "履道, start_char: 16, end_char: 18.\n",
      "坦坦, start_char: 18, end_char: 20.\n",
      "幽人, start_char: 21, end_char: 23.\n",
      "贞吉, start_char: 23, end_char: 25.\n",
      "六, start_char: 26, end_char: 27.\n",
      "五, start_char: 27, end_char: 28.\n",
      "密云, start_char: 29, end_char: 31.\n",
      "不雨, start_char: 31, end_char: 33.\n",
      "自我, start_char: 34, end_char: 36.\n",
      "西郊, start_char: 36, end_char: 38.\n",
      "公弋, start_char: 39, end_char: 41.\n",
      "取彼, start_char: 41, end_char: 43.\n",
      "在, start_char: 43, end_char: 44.\n",
      "穴, start_char: 44, end_char: 45.\n",
      "九, start_char: 46, end_char: 47.\n",
      "五, start_char: 47, end_char: 48.\n",
      "东邻, start_char: 49, end_char: 51.\n",
      "杀牛, start_char: 51, end_char: 53.\n",
      "不如, start_char: 54, end_char: 56.\n",
      "西邻, start_char: 56, end_char: 58.\n",
      "之, start_char: 58, end_char: 59.\n",
      "禴, start_char: 59, end_char: 60.\n",
      "祭, start_char: 60, end_char: 61.\n",
      "实受, start_char: 62, end_char: 64.\n",
      "其, start_char: 64, end_char: 65.\n",
      "福, start_char: 65, end_char: 66.\n",
      "吉, start_char: 68, end_char: 69.\n"
     ]
    }
   ],
   "source": [
    "analyzer = StanzaChineseAnalyzer()\n",
    "\n",
    "# ChineseAnalyzer内部使用的是搜索引擎模式\n",
    "tokens = [t.text for t in analyzer(text)]\n",
    "print(\"StanzaChineseAnalyzer:\", \"/ \".join(tokens)) \n",
    "\n",
    "for t in analyzer(text):\n",
    "    print(f\"{t.text}, start_char: {t.startchar}, end_char: {t.endchar}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ec211-17cc-4d30-b07a-617eeaf2ca8e",
   "metadata": {},
   "source": [
    "## pynlpir分词\n",
    "不支持M1芯片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec35358-df9d-41f7-bbd4-3db507a25161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pynlpir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28cfb5a-b363-420a-bd42-7d6641c482aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pynlpir\n",
    "#pynlpir.open()\n",
    "\n",
    "#pynlpir.segment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e56c4-fd5f-4d9f-b6a9-6ed7633bac20",
   "metadata": {},
   "source": [
    "## 甲言分词\n",
    "甲言分词，是做文言文分词的。经过测试，分词还算比较准确，但对九二，九五等分词，不太准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "fa896ccc-ffd1-408b-9731-5a5bc101c1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install jiayan\n",
    "#!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19c5ff2a-0ec1-45f3-9250-6833298cc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sunyafu/miniforge3/envs/pytorchpy310/lib/python3.10/site-packages\n",
      "/Users/sunyafu/miniforge3/envs/pytorchpy310/lib/python3.10/site-packages/jiayan/data/jiayan.klm\n",
      "\n",
      "1. Jiayan CharHMMTokenizer: 九/ 二/ ：/ 眇能视/ ，/ 利/ 幽人/ 之/ 贞/ 。/ 九/ 二/ ：/ 履道/ 坦坦/ ，/ 幽人/ 贞吉/ 。/ 六/ 五/ ：/ 密云/ 不/ 雨/ ，/ 自/ 我/ 西郊/ ；/ 公/ 弋/ 取/ 彼/ 在/ 穴/ 。/ 九五/ ：/ 东邻/ 杀牛/ ，/ 不/ 如/ 西邻/ 之/ 禴祭/ ，/ 实/ 受/ 其/ 福/ ，/ （/ 吉/ ）/ 。\n",
      "\n",
      "2. Jiayan WordNgramTokenizer: 九/ 二/ ：/ 眇/ 能/ 视/ ，/ 利/ 幽/ 人/ 之/ 贞/ 。/ 九/ 二/ ：/ 履/ 道/ 坦/ 坦/ ，/ 幽/ 人/ 贞吉/ 。/ 六/ 五/ ：/ 密/ 云/ 不/ 雨/ ，/ 自/ 我/ 西/ 郊/ ；/ 公/ 弋/ 取/ 彼/ 在/ 穴/ 。/ 九/ 五/ ：/ 东/ 邻/ 杀/ 牛/ ，/ 不/ 如/ 西/ 邻/ 之/ 禴/ 祭/ ，/ 实/ 受/ 其/ 福/ ，（/ 吉/ ）。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import site\n",
    "from jiayan import load_lm\n",
    "from jiayan import CharHMMTokenizer\n",
    "from jiayan import WordNgramTokenizer\n",
    "\n",
    "print(os.path.abspath(site.getsitepackages()[0]))\n",
    "model_path = os.path.join(site.getsitepackages()[0], 'jiayan', 'data', 'jiayan.klm') #site.getusersitepackages()\n",
    "print(model_path)\n",
    "\n",
    "# 字符级隐马尔可夫模型分词，效果符合语感，建议使用，需加载语言模型 jiayan.klm\n",
    "lm = load_lm(model_path)\n",
    "tokenizer = CharHMMTokenizer(lm)\n",
    "result = tokenizer.tokenize(text)\n",
    "\n",
    "tokens = [t for t in result]\n",
    "print(\"\\n1. Jiayan CharHMMTokenizer:\", \"/ \".join(tokens)) \n",
    "\n",
    "# 词级最大概率路径分词，基本以字为单位，颗粒度较粗\n",
    "\n",
    "tokenizer = WordNgramTokenizer()\n",
    "result = tokenizer.tokenize(text)\n",
    "\n",
    "tokens = [t for t in result]\n",
    "print(\"\\n2. Jiayan WordNgramTokenizer:\", \"/ \".join(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20a00f4b-35b9-4149-8f38-d7295aaab74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from whoosh.analysis import RegexAnalyzer, LowercaseFilter, StopFilter, StemFilter\n",
    "from whoosh.analysis import Tokenizer, Token\n",
    "from whoosh.lang.porter import stem\n",
    "from jiayan import load_lm\n",
    "from jiayan import CharHMMTokenizer\n",
    "import site\n",
    "import re\n",
    "\n",
    "STOP_WORDS = frozenset(('a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'can',\n",
    "                        'for', 'from', 'have', 'if', 'in', 'is', 'it', 'may',\n",
    "                        'not', 'of', 'on', 'or', 'tbd', 'that', 'the', 'this',\n",
    "                        'to', 'us', 'we', 'when', 'will', 'with', 'yet',\n",
    "                        'you', 'your', '的', '了', '和'))\n",
    "\n",
    "accepted_chars = re.compile(r\"[\\u4E00-\\u9FD5]+\")\n",
    "\n",
    "class JiayanChineseTokenizer(Tokenizer):\n",
    "    chartokenizer = None\n",
    "    wordtokenizer = None\n",
    "\n",
    "    def __call__(self, text, tokenizer = 'char', **kargs):\n",
    "        if tokenizer == 'word':\n",
    "            if self.wordtokenizer is None:\n",
    "                self.wordtokenizer = WordNgramTokenizer()\n",
    "            tokenizer = self.wordtokenizer\n",
    "        else:\n",
    "            if self.chartokenizer is None:\n",
    "                model_path = os.path.join(site.getsitepackages()[0], 'jiayan', 'data', 'jiayan.klm') #site.getusersitepackages()\n",
    "                lm = load_lm(model_path)\n",
    "                self.chartokenizer = CharHMMTokenizer(lm)\n",
    "            tokenizer = self.chartokenizer\n",
    "\n",
    "        result = tokenizer.tokenize(text)\n",
    "\n",
    "        count = 0\n",
    "        token = Token()\n",
    "        for tok in result:\n",
    "            text = tok\n",
    "            start_char = count\n",
    "            count = count + len(tok)\n",
    "            end_char = count\n",
    "\n",
    "            if not accepted_chars.match(text) and len(text) <= 1:\n",
    "                continue\n",
    "            token.original = token.text = text\n",
    "            token.pos = start_char\n",
    "            token.startchar = start_char\n",
    "            token.endchar = end_char\n",
    "            yield token\n",
    "\n",
    "def JiayanChineseAnalyzer(stoplist=STOP_WORDS, minsize=1, stemfn=stem, cachesize=50000):\n",
    "    return (JiayanChineseTokenizer() | LowercaseFilter() |\n",
    "            StopFilter(stoplist=stoplist, minsize=minsize) |\n",
    "            StemFilter(stemfn=stemfn, ignore=None, cachesize=cachesize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db1abfee-2b1a-4fac-b5a3-f328c6916054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. JiayanChineseAnalyzer(char): 九/ 二/ 眇能视/ 利/ 幽人/ 之/ 贞/ 九/ 二/ 履道/ 坦坦/ 幽人/ 贞吉/ 六/ 五/ 密云/ 不/ 雨/ 自/ 我/ 西郊/ 公/ 弋/ 取/ 彼/ 在/ 穴/ 九五/ 东邻/ 杀牛/ 不/ 如/ 西邻/ 之/ 禴祭/ 实/ 受/ 其/ 福/ 吉\n",
      "\n",
      "2. JiayanChineseAnalyzer(word): 九/ 二/ 眇/ 能/ 视/ 利/ 幽/ 人/ 之/ 贞/ 九/ 二/ 履/ 道/ 坦/ 坦/ 幽/ 人/ 贞吉/ 六/ 五/ 密/ 云/ 不/ 雨/ 自/ 我/ 西/ 郊/ 公/ 弋/ 取/ 彼/ 在/ 穴/ 九/ 五/ 东/ 邻/ 杀/ 牛/ 不/ 如/ 西/ 邻/ 之/ 禴/ 祭/ 实/ 受/ 其/ 福/ ，（/ 吉/ ）。\n",
      "九, start_char: 0, end_char: 1.\n",
      "二, start_char: 1, end_char: 2.\n",
      "眇, start_char: 3, end_char: 4.\n",
      "能, start_char: 4, end_char: 5.\n",
      "视, start_char: 5, end_char: 6.\n",
      "利, start_char: 7, end_char: 8.\n",
      "幽, start_char: 8, end_char: 9.\n",
      "人, start_char: 9, end_char: 10.\n",
      "之, start_char: 10, end_char: 11.\n",
      "贞, start_char: 11, end_char: 12.\n",
      "九, start_char: 13, end_char: 14.\n",
      "二, start_char: 14, end_char: 15.\n",
      "履, start_char: 16, end_char: 17.\n",
      "道, start_char: 17, end_char: 18.\n",
      "坦, start_char: 18, end_char: 19.\n",
      "坦, start_char: 19, end_char: 20.\n",
      "幽, start_char: 21, end_char: 22.\n",
      "人, start_char: 22, end_char: 23.\n",
      "贞吉, start_char: 23, end_char: 25.\n",
      "六, start_char: 26, end_char: 27.\n",
      "五, start_char: 27, end_char: 28.\n",
      "密, start_char: 29, end_char: 30.\n",
      "云, start_char: 30, end_char: 31.\n",
      "不, start_char: 31, end_char: 32.\n",
      "雨, start_char: 32, end_char: 33.\n",
      "自, start_char: 34, end_char: 35.\n",
      "我, start_char: 35, end_char: 36.\n",
      "西, start_char: 36, end_char: 37.\n",
      "郊, start_char: 37, end_char: 38.\n",
      "公, start_char: 39, end_char: 40.\n",
      "弋, start_char: 40, end_char: 41.\n",
      "取, start_char: 41, end_char: 42.\n",
      "彼, start_char: 42, end_char: 43.\n",
      "在, start_char: 43, end_char: 44.\n",
      "穴, start_char: 44, end_char: 45.\n",
      "九, start_char: 46, end_char: 47.\n",
      "五, start_char: 47, end_char: 48.\n",
      "东, start_char: 49, end_char: 50.\n",
      "邻, start_char: 50, end_char: 51.\n",
      "杀, start_char: 51, end_char: 52.\n",
      "牛, start_char: 52, end_char: 53.\n",
      "不, start_char: 54, end_char: 55.\n",
      "如, start_char: 55, end_char: 56.\n",
      "西, start_char: 56, end_char: 57.\n",
      "邻, start_char: 57, end_char: 58.\n",
      "之, start_char: 58, end_char: 59.\n",
      "禴, start_char: 59, end_char: 60.\n",
      "祭, start_char: 60, end_char: 61.\n",
      "实, start_char: 62, end_char: 63.\n",
      "受, start_char: 63, end_char: 64.\n",
      "其, start_char: 64, end_char: 65.\n",
      "福, start_char: 65, end_char: 66.\n",
      "，（, start_char: 66, end_char: 68.\n",
      "吉, start_char: 68, end_char: 69.\n",
      "）。, start_char: 69, end_char: 71.\n"
     ]
    }
   ],
   "source": [
    "analyzer = JiayanChineseAnalyzer()\n",
    "\n",
    "# JiayanChineseAnalyzer CharHMMTokenizer\n",
    "tokens = [t.text for t in analyzer(text)]\n",
    "print(\"1. JiayanChineseAnalyzer(char):\", \"/ \".join(tokens))\n",
    "\n",
    "# JiayanChineseAnalyzer WordNgramTokenizer\n",
    "tokens = [t.text for t in analyzer(text, tokenizer = 'word')]\n",
    "print(\"\\n2. JiayanChineseAnalyzer(word):\", \"/ \".join(tokens))\n",
    "\n",
    "for t in analyzer(text, tokenizer = 'word'):\n",
    "    print(f\"{t.text}, start_char: {t.startchar}, end_char: {t.endchar}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ffc56-1003-4735-87de-f3e605e1eaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205b081-d3bb-4d30-83c7-b6a4c0af8df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
